### write concerns

Default:

w: acknowledge write to cache

w = 1

j: wait for the journal cache to be written to disk

j = 0

Provided you assume that the disk is persistent, what are the w and j settings required to guarantee that an insert or update has been written all the way to disk.

w=1, j=1

Write concern (w) value can be set at client, database or collection level within PyMongo.

When you call MongoClient, you get a connection to the driver, but behind the scenes,
PyMongo connects to multiple nodes of the replica set.

The w value can be set at the client level.

Andrew says that the w concern can be set at the connection level;
he really means client level.

It's also important to note that wtimeout is the amount of time that the database will wait for replication before returning an error on the driver, but that even if the database returns an error due to wtimeout,
the write will not be unwound at the primary and may complete at the secondaries.

Hence, writes that return errors to the client due to wtimeout may in fact succeed,
but writes that return success, do in fact succeed.

w: 'majority'
j: is only on the Primary

### replication

Election: When voting for a new Primary there needs to bee a clear majority on the vote.

What is the minimum original number of nodes needed to assure the election of a new Primary if a node goes down?

3

Which types of nodes can participate in elections of a new primary?

Regular replica set members
Hidden Members
Arbiters


rs.slaveOK() #OK to read from the secondary


use local
db.oplog.rs.find.pretty()

rs.status()
rs.stepDown()
rs.conf()
rs.help()

### REPLICA SET INTERNALS

Replication supports mixed-mode storage engines. For examples, a mmapv1 primary and wiredTiger secondary.

A copy of the oplog is kept on both the primary and secondary servers.

The oplog is implemented as a capped collection.


### Failover and Rollback


It is possible that a write performed with w=majority gets rolled back.

Here is the scenario: you do write with w=majority and a failover over occurs after the write has committed to the primary but before replication completes. You will likely see an exception at the client. An election occurs and a new primary is elected. When the original primary comes back up, it will rollback the committed write. However, from your application's standpoint, that write never completed, so that's ok.


What happens if a node comes back up as a secondary after a period of being offline and the oplog has looped on the primary?

The entire dataset will be copied from the primary


### Connecting ti a replica set from Pymongo

seed list in the pymongo.MongoClient()


If you leave a replica set node out of the seedlist within the driver, what will happen?

The missing node will be discovered as long as you list at least one valid node.


What will happen if the following statement is executed in Python during a primary election?

db.test.insert_one({'x':1})

Insert will fail, program will terminate, if not the exception is handled.

pymongo.errors.AutoReconnect
pymongo.errors.DuplicateKeyError

Note that pymongo.errors.AutoReconnect tries to reconnect in the background.


### read prefernce

primary
primaryPrefered
secondary
SecondaryPrefered
Nearest

TAGS


### sharding

connect to a mongos, instead of mongod

range based or hash based, chunks

shard key, can be any kind of id



### QUESTIONS

Which of the following statements are true about replication in MongoDB? Check all that apply.

The minimum sensible number of voting nodes to a replica set is three.
By default, using the MongoClient connection class, w=1 and j=0.
The oplog utilizes a capped collection.

Which of the following statements are true about choosing and using a shard key:



Which of the following statements are true about choosing and using a shard key?


There must be a index on the collection that starts with the shard key.
Any update that does not contain the shard key will be sent to all shards.
MongoDB can not enforce unique indexes on a sharded collection other than the shard key itself, or indexes prefixed by the shard key.
